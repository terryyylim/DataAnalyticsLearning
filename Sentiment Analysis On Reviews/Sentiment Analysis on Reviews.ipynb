{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews sentiment analysis\n",
    "This dataset contains reviews for various purposes and services, ranging from car tyre repairs, restaurant customer service to hotel stay service. Since the instructions on Kaggle are rather unclear, I decided to first categorize the data into 1 (positive sentiment) and 0 (negative sentiment). \n",
    "\n",
    "It is also good to take note that, rating grades 1,2,3 are considered negative sentiments while rating grades 4,5 are considered positive sentiments.\n",
    "\n",
    "We will be using NLTK's Naive Bayes Classifier to classify the sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re #regex methods\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from os.path import join\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Retrieving our training dataset\n",
    "train_x = pd.read_csv(\"train.csv\", sep='delimiter', header=None, engine='python', names=['Initial Review'])\n",
    "#print(train_x)\n",
    "\n",
    "# Retrieving our test dataset\n",
    "test_x = pd.read_csv(\"test.csv\", sep='delimiter', header=None, engine='python', names=['Initial Review'])\n",
    "#print(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a new empty column to store Rating Grades\n",
    "train_x['Rating'] = ''\n",
    "\n",
    "# Extracting rating grades into Rating column\n",
    "for i in range(len(train_x['Initial Review'])):\n",
    "    train_x['Rating'][i] = train_x['Initial Review'][i][0]\n",
    "train_x['Rating'] = train_x['Rating'].astype(int)\n",
    "\n",
    "# Creating a new empty column for sentiments\n",
    "train_x['Sentiment'] = ''\n",
    "train_x.loc[train_x['Rating'] >= 4, 'Sentiment'] = 1\n",
    "train_x.loc[train_x['Rating'] <= 3, 'Sentiment'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, after categorizing the reviews into either positive or negative sentiments, let's proceed with \"cleaning\" the data, extracting the text content through tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\tThank you thank you thank you !! I  want to thank the people that made this place happen ....you have made all my dreams come true. Imagine a delicious yogurt shop with super fun flavors like peanut butter, chocolate mint, cake batter and so many more.  I used to have to travel to  Yogurtland or Jujuberry  but not anymore , now we have one right in the hood!! Guess what?   instead of eating a normal lunch I can pig out with a healthy peanut butter yogurt smothered in chocolate chips.   Could be the perfect lunch!!  See you there!\n"
     ]
    }
   ],
   "source": [
    "print(train_x['Initial Review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Terence\n",
      "[nltk_data]     Lim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing nltk library\n",
    "import nltk\n",
    "\n",
    "# Download only 'stopwords' resource and not nltk.download() as it could take very long\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# List of stopwords\n",
    "stopwords_dict = stopwords.words('english')\n",
    "\n",
    "print()\n",
    "\n",
    "# Creating a list of empty lists\n",
    "list_of_emptylists = []\n",
    "for i in range(len(train_x['Initial Review'])):\n",
    "    list_of_emptylists.append([])\n",
    "    \n",
    "# Creating a new column to store individual words in each review (training data)\n",
    "train_x['Individual Words'] = list_of_emptylists\n",
    "\n",
    "for sentence in range(len(train_x['Initial Review'])):\n",
    "    # Regex removal of punctuations\n",
    "    splitted_sentence = re.split(r'\\W+', train_x['Initial Review'][sentence].lower())\n",
    "\n",
    "    # New list of words to store words and ensure no repeated storage\n",
    "    for i in range(1,len(splitted_sentence)):\n",
    "        if splitted_sentence[i] not in train_x['Individual Words'][sentence] and splitted_sentence[i] != '' and splitted_sentence[i] not in stopwords_dict:\n",
    "            train_x['Individual Words'][sentence].append(splitted_sentence[i])\n",
    "#train_x['Individual Words'][sentence] = train_x['Individual Words'][sentence][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of empty lists\n",
    "list_of_emptylists_test = []\n",
    "for i in range(len(test_x['Initial Review'])):\n",
    "    list_of_emptylists_test.append([])\n",
    "\n",
    "# Creating a new column to store individual words in each review (test data)\n",
    "test_x['Individual Words'] = list_of_emptylists_test\n",
    "\n",
    "for sentence in range(len(test_x['Initial Review'])):\n",
    "    # Regex removal of punctuations\n",
    "    splitted_sentence = re.split(r'\\W+', test_x['Initial Review'][sentence].lower())\n",
    "    \n",
    "    # New list of words to store words and ensure no repeated storage\n",
    "    for i in range(1,len(splitted_sentence)):\n",
    "        if splitted_sentence[i] not in test_x['Individual Words'][sentence] and splitted_sentence[i] != '' and splitted_sentence[i] not in stopwords_dict:\n",
    "            test_x['Individual Words'][sentence].append(splitted_sentence[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since stemming can often create non-existent words, I decided to go along with lemmatizing (where outputs are actual words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Terence\n",
      "[nltk_data]     Lim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing required library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download only 'wordnet' resource\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Syntax for using Stemmer if you want to check out how it works\n",
    "#porter = PorterStemmer()\n",
    "#print(porter.stem('smothered'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize sentences in training data\n",
    "for sentence in range(len(train_x['Individual Words'])):\n",
    "    for word in range(len(train_x['Individual Words'][sentence])):\n",
    "        train_x['Individual Words'][sentence][word] = lemmatizer.lemmatize(train_x['Individual Words'][sentence][word], 'v')\n",
    "\n",
    "# Lemmatize sentences in test data\n",
    "for sentence in range(len(test_x['Individual Words'])):\n",
    "    for word in range(len(test_x['Individual Words'][sentence])):\n",
    "        test_x['Individual Words'][sentence][word] = lemmatizer.lemmatize(test_x['Individual Words'][sentence][word], 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are building a Naive Bayes classifier using NLTK library, we need to ensure that the input is in the format where, every word is followed by true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29351\n",
      "52714\n",
      "57446\n",
      "24619\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Converting to a form where it fits how the Naive Bayes classifier expects the input\n",
    "# A dictionary is used because if a word already exists, it will not be added to the dictionary\n",
    "def create_word_features(words):\n",
    "    my_dict = dict([(word, True) for word in words])\n",
    "    return my_dict\n",
    "\n",
    "# Creating a list to store negative and positive reviews\n",
    "neg_reviews = []\n",
    "pos_reviews = []\n",
    "for sentence in range(len(train_x['Initial Review'])):\n",
    "    if train_x['Sentiment'][sentence] == 0:\n",
    "        neg_reviews.append((create_word_features(train_x['Individual Words'][sentence]), \"negative\"))\n",
    "    elif train_x['Sentiment'][sentence] == 1:\n",
    "        pos_reviews.append((create_word_features(train_x['Individual Words'][sentence]), \"positive\"))\n",
    "\n",
    "# Splitting into training and test dataset\n",
    "train_set = neg_reviews[:20546] + pos_reviews[:36900]\n",
    "test_set = neg_reviews[20546:] + pos_reviews[36900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:71.31890003655712%\n"
     ]
    }
   ],
   "source": [
    "# Creating the Naive Bayes Classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test_set)\n",
    "print(\"Accuracy:\" + str(accuracy*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After achieving an accuracy rate of 71.3% which is pretty decent, there are still various areas that can be improved. An area that needs improvement is definitely when splitting the words. More time and effort can be put into understanding the dataset and making it more suitable to run sentiment analysis on. \n",
    "\n",
    "Some possible areas of exploration for sentiment analysis would be Beautiful Soup as well as TfidfVectorizer. Till then, this shall be a simple Naive Bayes Classification model for classifying sentiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network Classifier\n",
    "\n",
    "An artificial neuron network (ANN) is a computational model based on the structure and functions of biological neural networks. Information that flows through the network affects the structure of the ANN because a neural network learns based on the input and output.\n",
    "\n",
    "Each input is multiplied by its corresponding weights. Weights are the information used by the neural network to solve a problem. Typically weight represents the strength of the interconnection between neurons inside the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "Dictionary is in the form of (variable):(definition)<br>\n",
    "1) survival: Survival (0 = No, 1 = Yes)<br>\n",
    "2) pclass: Ticket class\t(1 = 1st, 2 = 2nd, 3 = 3rd)<br>\n",
    "3) sex: Sex\t<br>\n",
    "4) Age: Age in years\t\n",
    "5) sibsp: # of siblings / spouses aboard the Titanic\t\n",
    "6) parch: # of parents / children aboard the Titanic\t\n",
    "7) ticket: Ticket number\t\n",
    "8) fare: Passenger fare\t\n",
    "9) cabin: Cabin number\t\n",
    "10) embarked: Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "\n",
    "## Variable Notes\n",
    "1) pclass: A proxy for socio-economic status (SES)<br>\n",
    "<li>1st = Upper</li>\n",
    "<li>2nd = Middle</li>\n",
    "<li>3rd = Lower</li>\n",
    "\n",
    "2) age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "\n",
    "3) sibsp: The dataset defines family relations in this way...\n",
    "<li>Sibling = brother, sister, stepbrother, stepsister</li>\n",
    "<li>Spouse = husband, wife (mistresses and fiancés were ignored)</li>\n",
    "\n",
    "4) parch: The dataset defines family relations in this way...<br>\n",
    "<li>Parent = mother, father</li>\n",
    "<li>Child = daughter, son, stepdaughter, stepson</li>\n",
    "<li>Some children travelled only with a nanny, therefore parch=0 for them.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "5              6         0       3   \n",
      "6              7         0       1   \n",
      "7              8         0       3   \n",
      "8              9         1       3   \n",
      "9             10         1       2   \n",
      "10            11         1       3   \n",
      "11            12         1       1   \n",
      "12            13         0       3   \n",
      "13            14         0       3   \n",
      "14            15         0       3   \n",
      "15            16         1       2   \n",
      "16            17         0       3   \n",
      "17            18         1       2   \n",
      "18            19         0       3   \n",
      "19            20         1       3   \n",
      "20            21         0       2   \n",
      "21            22         1       2   \n",
      "22            23         1       3   \n",
      "23            24         1       1   \n",
      "24            25         0       3   \n",
      "25            26         1       3   \n",
      "26            27         0       3   \n",
      "27            28         0       1   \n",
      "28            29         1       3   \n",
      "29            30         0       3   \n",
      "..           ...       ...     ...   \n",
      "861          862         0       2   \n",
      "862          863         1       1   \n",
      "863          864         0       3   \n",
      "864          865         0       2   \n",
      "865          866         1       2   \n",
      "866          867         1       2   \n",
      "867          868         0       1   \n",
      "868          869         0       3   \n",
      "869          870         1       3   \n",
      "870          871         0       3   \n",
      "871          872         1       1   \n",
      "872          873         0       1   \n",
      "873          874         0       3   \n",
      "874          875         1       2   \n",
      "875          876         1       3   \n",
      "876          877         0       3   \n",
      "877          878         0       3   \n",
      "878          879         0       3   \n",
      "879          880         1       1   \n",
      "880          881         1       2   \n",
      "881          882         0       3   \n",
      "882          883         0       3   \n",
      "883          884         0       2   \n",
      "884          885         0       3   \n",
      "885          886         0       3   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                             Allen, Mr. William Henry    male  35.0      0   \n",
      "5                                     Moran, Mr. James    male   NaN      0   \n",
      "6                              McCarthy, Mr. Timothy J    male  54.0      0   \n",
      "7                       Palsson, Master. Gosta Leonard    male   2.0      3   \n",
      "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
      "9                  Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
      "10                     Sandstrom, Miss. Marguerite Rut  female   4.0      1   \n",
      "11                            Bonnell, Miss. Elizabeth  female  58.0      0   \n",
      "12                      Saundercock, Mr. William Henry    male  20.0      0   \n",
      "13                         Andersson, Mr. Anders Johan    male  39.0      1   \n",
      "14                Vestrom, Miss. Hulda Amanda Adolfina  female  14.0      0   \n",
      "15                    Hewlett, Mrs. (Mary D Kingcome)   female  55.0      0   \n",
      "16                                Rice, Master. Eugene    male   2.0      4   \n",
      "17                        Williams, Mr. Charles Eugene    male   NaN      0   \n",
      "18   Vander Planke, Mrs. Julius (Emelia Maria Vande...  female  31.0      1   \n",
      "19                             Masselmani, Mrs. Fatima  female   NaN      0   \n",
      "20                                Fynney, Mr. Joseph J    male  35.0      0   \n",
      "21                               Beesley, Mr. Lawrence    male  34.0      0   \n",
      "22                         McGowan, Miss. Anna \"Annie\"  female  15.0      0   \n",
      "23                        Sloper, Mr. William Thompson    male  28.0      0   \n",
      "24                       Palsson, Miss. Torborg Danira  female   8.0      3   \n",
      "25   Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...  female  38.0      1   \n",
      "26                             Emir, Mr. Farred Chehab    male   NaN      0   \n",
      "27                      Fortune, Mr. Charles Alexander    male  19.0      3   \n",
      "28                       O'Dwyer, Miss. Ellen \"Nellie\"  female   NaN      0   \n",
      "29                                 Todoroff, Mr. Lalio    male   NaN      0   \n",
      "..                                                 ...     ...   ...    ...   \n",
      "861                        Giles, Mr. Frederick Edward    male  21.0      1   \n",
      "862  Swift, Mrs. Frederick Joel (Margaret Welles Ba...  female  48.0      0   \n",
      "863                  Sage, Miss. Dorothy Edith \"Dolly\"  female   NaN      8   \n",
      "864                             Gill, Mr. John William    male  24.0      0   \n",
      "865                           Bystrom, Mrs. (Karolina)  female  42.0      0   \n",
      "866                       Duran y More, Miss. Asuncion  female  27.0      1   \n",
      "867               Roebling, Mr. Washington Augustus II    male  31.0      0   \n",
      "868                        van Melkebeke, Mr. Philemon    male   NaN      0   \n",
      "869                    Johnson, Master. Harold Theodor    male   4.0      1   \n",
      "870                                  Balkic, Mr. Cerin    male  26.0      0   \n",
      "871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)  female  47.0      1   \n",
      "872                           Carlsson, Mr. Frans Olof    male  33.0      0   \n",
      "873                        Vander Cruyssen, Mr. Victor    male  47.0      0   \n",
      "874              Abelson, Mrs. Samuel (Hannah Wizosky)  female  28.0      1   \n",
      "875                   Najib, Miss. Adele Kiamie \"Jane\"  female  15.0      0   \n",
      "876                      Gustafsson, Mr. Alfred Ossian    male  20.0      0   \n",
      "877                               Petroff, Mr. Nedelio    male  19.0      0   \n",
      "878                                 Laleff, Mr. Kristo    male   NaN      0   \n",
      "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)  female  56.0      0   \n",
      "880       Shelley, Mrs. William (Imanita Parrish Hall)  female  25.0      0   \n",
      "881                                 Markun, Mr. Johann    male  33.0      0   \n",
      "882                       Dahlberg, Miss. Gerda Ulrika  female  22.0      0   \n",
      "883                      Banfield, Mr. Frederick James    male  28.0      0   \n",
      "884                             Sutehall, Mr. Henry Jr    male  25.0      0   \n",
      "885               Rice, Mrs. William (Margaret Norton)  female  39.0      0   \n",
      "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
      "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
      "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
      "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
      "\n",
      "     Parch            Ticket      Fare        Cabin Embarked  \n",
      "0        0         A/5 21171    7.2500          NaN        S  \n",
      "1        0          PC 17599   71.2833          C85        C  \n",
      "2        0  STON/O2. 3101282    7.9250          NaN        S  \n",
      "3        0            113803   53.1000         C123        S  \n",
      "4        0            373450    8.0500          NaN        S  \n",
      "5        0            330877    8.4583          NaN        Q  \n",
      "6        0             17463   51.8625          E46        S  \n",
      "7        1            349909   21.0750          NaN        S  \n",
      "8        2            347742   11.1333          NaN        S  \n",
      "9        0            237736   30.0708          NaN        C  \n",
      "10       1           PP 9549   16.7000           G6        S  \n",
      "11       0            113783   26.5500         C103        S  \n",
      "12       0         A/5. 2151    8.0500          NaN        S  \n",
      "13       5            347082   31.2750          NaN        S  \n",
      "14       0            350406    7.8542          NaN        S  \n",
      "15       0            248706   16.0000          NaN        S  \n",
      "16       1            382652   29.1250          NaN        Q  \n",
      "17       0            244373   13.0000          NaN        S  \n",
      "18       0            345763   18.0000          NaN        S  \n",
      "19       0              2649    7.2250          NaN        C  \n",
      "20       0            239865   26.0000          NaN        S  \n",
      "21       0            248698   13.0000          D56        S  \n",
      "22       0            330923    8.0292          NaN        Q  \n",
      "23       0            113788   35.5000           A6        S  \n",
      "24       1            349909   21.0750          NaN        S  \n",
      "25       5            347077   31.3875          NaN        S  \n",
      "26       0              2631    7.2250          NaN        C  \n",
      "27       2             19950  263.0000  C23 C25 C27        S  \n",
      "28       0            330959    7.8792          NaN        Q  \n",
      "29       0            349216    7.8958          NaN        S  \n",
      "..     ...               ...       ...          ...      ...  \n",
      "861      0             28134   11.5000          NaN        S  \n",
      "862      0             17466   25.9292          D17        S  \n",
      "863      2          CA. 2343   69.5500          NaN        S  \n",
      "864      0            233866   13.0000          NaN        S  \n",
      "865      0            236852   13.0000          NaN        S  \n",
      "866      0     SC/PARIS 2149   13.8583          NaN        C  \n",
      "867      0          PC 17590   50.4958          A24        S  \n",
      "868      0            345777    9.5000          NaN        S  \n",
      "869      1            347742   11.1333          NaN        S  \n",
      "870      0            349248    7.8958          NaN        S  \n",
      "871      1             11751   52.5542          D35        S  \n",
      "872      0               695    5.0000  B51 B53 B55        S  \n",
      "873      0            345765    9.0000          NaN        S  \n",
      "874      0         P/PP 3381   24.0000          NaN        C  \n",
      "875      0              2667    7.2250          NaN        C  \n",
      "876      0              7534    9.8458          NaN        S  \n",
      "877      0            349212    7.8958          NaN        S  \n",
      "878      0            349217    7.8958          NaN        S  \n",
      "879      1             11767   83.1583          C50        C  \n",
      "880      1            230433   26.0000          NaN        S  \n",
      "881      0            349257    7.8958          NaN        S  \n",
      "882      0              7552   10.5167          NaN        S  \n",
      "883      0  C.A./SOTON 34068   10.5000          NaN        S  \n",
      "884      0   SOTON/OQ 392076    7.0500          NaN        S  \n",
      "885      5            382652   29.1250          NaN        Q  \n",
      "886      0            211536   13.0000          NaN        S  \n",
      "887      0            112053   30.0000          B42        S  \n",
      "888      2        W./C. 6607   23.4500          NaN        S  \n",
      "889      0            111369   30.0000         C148        C  \n",
      "890      0            370376    7.7500          NaN        Q  \n",
      "\n",
      "[891 rows x 12 columns]\n",
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ggplot import *\n",
    "\n",
    "# Importing the datasets we need\n",
    "df_train = pd.read_csv('train.csv')\n",
    "print(df_train)\n",
    "\n",
    "# To know what type of data we are dealing with in the dataset\n",
    "print(df_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Preprocessing (Cleaning our dataset)\n",
    "Before we begin building our ANN, let us go through the dataset and understand what attributes should we place our emphasis on. For attributes such as ticket number which is probably related to the class of the passenger and port of embarkation, which have little to zero relevance to survival of a passenger, we shall remove them from the dataset. \n",
    "\n",
    "Also, we can consider removing rows with missing values. For me, I removed rows of data with missing data in order to avoid adding unnecessary noise to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column\n",
      "Survived      0\n",
      "Pclass        0\n",
      "Sex           0\n",
      "Age         177\n",
      "Fare          0\n",
      "Cabin       687\n",
      "Family        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Deleting name, ticket and port of embarkation columns from dataset\n",
    "del df_train['Name']\n",
    "del df_train['Ticket']\n",
    "del df_train['Embarked']\n",
    "del df_train['PassengerId']\n",
    "\n",
    "# Adding both 'SibSb' and 'Parch' columns together because all of them probably travelled together as a family\n",
    "df_train['Family'] = df_train['SibSp'] + df_train['Parch']\n",
    "del df_train['SibSp']\n",
    "del df_train['Parch']\n",
    "\n",
    "# Find number of missing values in each column\n",
    "def num_missing(x):\n",
    "    return sum(x.isnull())\n",
    "print(\"Missing values per column\")\n",
    "print(df_train.apply(num_missing, axis=0))\n",
    "    \n",
    "# Remove cabin column as well since 687 out of 891 rows have no values\n",
    "del df_train['Cabin']\n",
    "\n",
    "# Removal of rows with missing age values\n",
    "df_train = df_train.dropna(axis=0)\n",
    "\n",
    "# Lets get the matrix of target variables i.e 'Survived' column\n",
    "df_target = df_train['Survived']\n",
    "del df_train['Survived']\n",
    "\n",
    "# Creating matrix of features and matrix of target variable\n",
    "#df_train = df_train.iloc[:].values\n",
    "#df_target = df_target.iloc[:].values\n",
    "\n",
    "# Making analysis simpler by encoding string variables\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Replacing male=1 and female=0 for Sex column since ANN can only run with float value data\n",
    "\"\"\"labelencoder_X_1 = LabelEncoder()\n",
    "df_train[:, 1] = labelencoder_X_1.fit_transform(df_train[:, 1])\"\"\"\n",
    "df_train['Sex'] = df_train['Sex'].replace(['male','female'], [1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using scikit learn library\n",
    "As we carefully observe the data, we will realise that the data is not properly scaled. Some variables have value in hundreds while some have values that are tens or ones. Since we do not want any of our variable to dominate one another, let's go and scale the data.\n",
    "\n",
    "StandardScaler assumes that our data is normally distributed within each feature and will scale them such that the distribution is now centered around 0, with a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.91123237  0.75905134 -0.53037664 -0.51897787  0.03778282]\n",
      " [-1.47636364 -1.31743394  0.57183099  0.69189675  0.03778282]\n",
      " [ 0.91123237 -1.31743394 -0.25482473 -0.50621356 -0.63664053]\n",
      " ..., \n",
      " [-1.47636364 -1.31743394 -0.73704057 -0.08877362 -0.63664053]\n",
      " [-1.47636364  0.75905134 -0.25482473 -0.08877362 -0.63664053]\n",
      " [ 0.91123237  0.75905134  0.15850313 -0.50952283 -0.63664053]]\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "df_train = sc.fit_transform(df_train)\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's repeat the process for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "[[ 0.91123237  0.75905134 -0.53037664 -0.51897787  0.03778282]\n",
      " [-1.47636364 -1.31743394  0.57183099  0.69189675  0.03778282]\n",
      " [ 0.91123237 -1.31743394 -0.25482473 -0.50621356 -0.63664053]\n",
      " ..., \n",
      " [-1.47636364 -1.31743394 -0.73704057 -0.08877362 -0.63664053]\n",
      " [-1.47636364  0.75905134 -0.25482473 -0.08877362 -0.63664053]\n",
      " [ 0.91123237  0.75905134  0.15850313 -0.50952283 -0.63664053]]\n",
      "[[ 1.01260812  0.79095876  0.31107954 -0.54385986 -0.63857217]\n",
      " [ 1.01260812 -1.26428842  1.19972436 -0.55741179  0.08310186]\n",
      " [-0.17235883  0.79095876  2.26609814 -0.51348896 -0.63857217]\n",
      " ..., \n",
      " [ 1.01260812 -1.26428842 -0.15101576 -0.54474567 -0.63857217]\n",
      " [-1.35732578 -1.26428842  0.63099168  1.10797855 -0.63857217]\n",
      " [ 1.01260812  0.79095876  0.59544588 -0.55332595 -0.63857217]]\n",
      "(714, 5) (714,) (330, 5) (330,)\n",
      "Train size: 714\n",
      "Test size: 330\n"
     ]
    }
   ],
   "source": [
    "# Retrieving test data\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Find missing values for test data\n",
    "print(\"Missing values per column\")\n",
    "print(df_test.apply(num_missing, axis=0))\n",
    "    \n",
    "# Deleting irrelevant columns from test data\n",
    "del df_test['Name']\n",
    "del df_test['Ticket']\n",
    "del df_test['Embarked']\n",
    "del df_test['Cabin']\n",
    "\n",
    "# Adding both 'SibSb' and 'Parch' columns together because all of them probably travelled together as a family\n",
    "df_test['Family'] = df_test['SibSp'] + df_test['Parch']\n",
    "del df_test['SibSp']\n",
    "del df_test['Parch']\n",
    "\n",
    "# Replace nan values with 0\n",
    "for column in df_test:\n",
    "    df_test[column].fillna(0, inplace=True)\n",
    "\n",
    "# Store indexes of rows to remove data from prediction results data\n",
    "index_to_remove = []\n",
    "\n",
    "for row in range(len(df_test['Age'])):\n",
    "    if df_test['Age'][row] == 0 or df_test['Fare'][row] == 0:\n",
    "        index_to_remove.append(row)\n",
    "\n",
    "# Removal of rows with missing age values\n",
    "df_test = df_test.loc[df_test['Age'] != 0]\n",
    "df_test = df_test.loc[df_test['Fare'] != 0]\n",
    "\n",
    "# Replacing male=1 and female=0 for Sex column since Naive Bayes model can only run with float value data\n",
    "df_test['Sex'] = df_test['Sex'].replace(['male','female'], [1,0])\n",
    "\n",
    "# Reverse the indexes of the row to ensure that python for-loop does not go out of index since we are removing rows\n",
    "index_to_remove = index_to_remove[::-1]\n",
    "\n",
    "# Retrieving test prediction data\n",
    "df_target_pred = pd.read_csv('gender_submission.csv')\n",
    "for i in range(len(index_to_remove)):\n",
    "    df_target_pred.drop(df_target_pred.index[index_to_remove[i]], inplace=True)\n",
    "\n",
    "del df_test['PassengerId']\n",
    "#del df_target_pred['PassengerId']\n",
    "df_target_pred2 = df_target_pred[\"Survived\"]\n",
    "\n",
    "# Feature scaling\n",
    "sc2 = StandardScaler()\n",
    "df_test = sc2.fit_transform(df_test)\n",
    "\n",
    "print(df_train)\n",
    "print(df_test)\n",
    "print(df_train.shape, df_target.shape, df_test.shape, df_target_pred2.shape)\n",
    "print('Train size:', df_train.shape[0])\n",
    "print('Test size:', df_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building the Neural Network now. We will need Sequential module for initializing the NN and dense module to add Hidden Layers.\n",
    "\n",
    "### Dense function\n",
    "Adding multiple hidden layers will take some effort. Each hidden layer will be added one by one using the dense function.\n",
    "The first parameter is <strong>output_dim</strong>. It is simply the number of nodes you want to add to this layer. <strong>init</strong> is the initialization of Stochastic Gradient Decent. In Neural Network we need to assign weights to each mode which is nothing but importance of that node. At the time of initialization, weights should be close to 0 and we will randomly initialize weights using uniform function. <strong>input_dim</strong> parameter is needed only for first layer as model doesn’t know the number of our input variables. Remember in our case, the total number of input variables are 11. In the second layer model automatically knows the number of input variable from the first hidden layer.\n",
    "\n",
    "### Activation function\n",
    "This is an important concept to understand. Neuron applies activation function to weighted sum(summation of Wi * Xi where w is weight, X is input variable and i is suffix of W and X). The closer the activation function value to 1 the more activated is the neuron and more the neuron passes the signal. <br><br>\n",
    "What is important, is deciding which activation function to be used. Here we are using rectifier(relu) function in our hidden layer and Sigmoid function in our output layer as we want binary result from output layer but if the number of categories in output layer is more than 2 then use SoftMax function.\n",
    "\n",
    "### Loss function\n",
    "Here, I chose to use 'binary_crossentropy' because for our output, we are trying to do a binary classification. Our output is the 'Survived' column of our dataset, where 1 represents 'Yes' and 2 represents 'No'.\n",
    "\n",
    "## Optimizer\n",
    "The traditional <strong>Batch Gradient Descent</strong> will calculate the gradient of the whole dataset, but will perform only one update, hence it can be very slow and hard to control for datasets which are very large and don't fit in the Memory. Also, it computes redundant updates for large datasets.<br>\n",
    "\n",
    "For <strong>Stochastic Gradient Descent</strong>(SGD), it performs a parameter update for each training example. This is usually much faster and it performs one update at a time. Now due to these frequent updates, parameters updates have high variance and causes the Loss function to fluctuate to different intensities. This is actually a good thing because it helps us discover new and possibly better local minima, whereas <strong>Standard Gradient Descent</strong> will only converge to the minimum of the basin as mentioned above.<br>\n",
    "\n",
    "But, the ultimate problem with SGD is that due to the frequent updates and fluctuations, it ultimately complicates the convergence to the exact minimum and will keep overshooting due to the frequent fluctuations.<br>\n",
    "To improve on this, we can use <strong>Mini Batch Gradient Descent</strong>, which reduces the variance in the parameter updates, and this can ultimately lead us to a much better and stable convergence.\n",
    "\n",
    "### Challenges faced while using Gradient Descent and its variants\n",
    "1) Difficulty in choosing a proper learning rate. A learning rate that is too small leads to painfully slow convergence, possibly resulting in the overall training time being too large.<br>\n",
    "\n",
    "2) Same learning rate is applied to all parameter updates. If our data is sparse, and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occuring features.<br>\n",
    "\n",
    "3) Minimizing highly non-convex error functions common for neural networks often result in being trapped in their numerous sub-optimal local minima. Additionally, saddle points are usually surrounded by a plateau of the same error, making it notoriously difficult for SGD to escape, as the gradient is close to zero in all dimensions.\n",
    "\n",
    "### Momentum\n",
    "Since high variance oscillations in SGD make it hard to reach convergence, a technique called Momentum was invented, which accelerates SGD by navigating along the relevant direction and softens the oscillations in irrelevant directions. It adds a fraction 'γ' of the update vector of the past step to the current update vector.\n",
    "\n",
    "The momentum term γ increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. This means it does parameter updates only for relevant examples. This reduces the unnecessary parameter updates which leads to faster and stable convergence and reduced oscillations.\n",
    "\n",
    "### Adam\n",
    "Adam stands for <strong>Adaptive Moment Estimation</strong>. It is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like AdaDelta, Adam also keeps an exponentially decaying average of past gradients M(t), similar to momentum. Adam works well in practice and compares favorably to other adaptive learning-method algorithms. This is because it converges very fast and the learning speed of the Model is quite fast and efficient and also it rectifies every problem that is faced in other optimization techniques such as vanishing Learning rate, slow convergence or High variance in the parameter updates which leads to fluctuating Loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 714 samples, validate on 330 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 0.6889 - acc: 0.5826 - val_loss: 0.6797 - val_acc: 0.6182\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.6736 - acc: 0.6148 - val_loss: 0.6528 - val_acc: 0.6848\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.6444 - acc: 0.7423 - val_loss: 0.6072 - val_acc: 0.8061\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.6021 - acc: 0.7815 - val_loss: 0.5469 - val_acc: 0.8091\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.5556 - acc: 0.7885 - val_loss: 0.4812 - val_acc: 0.8273\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.5141 - acc: 0.7927 - val_loss: 0.4194 - val_acc: 0.8636\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.4864 - acc: 0.7815 - val_loss: 0.3762 - val_acc: 0.8909\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.4674 - acc: 0.7885 - val_loss: 0.3487 - val_acc: 0.8848\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.4569 - acc: 0.7927 - val_loss: 0.3296 - val_acc: 0.8909\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.4499 - acc: 0.7997 - val_loss: 0.3132 - val_acc: 0.9061\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.4461 - acc: 0.7983 - val_loss: 0.3081 - val_acc: 0.9121\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.4426 - acc: 0.8011 - val_loss: 0.3056 - val_acc: 0.9000\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.4404 - acc: 0.8025 - val_loss: 0.2934 - val_acc: 0.9121\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.4368 - acc: 0.7969 - val_loss: 0.2931 - val_acc: 0.9121\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.4349 - acc: 0.8011 - val_loss: 0.2872 - val_acc: 0.9121\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.4347 - acc: 0.8053 - val_loss: 0.2958 - val_acc: 0.9000\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.4317 - acc: 0.8025 - val_loss: 0.2926 - val_acc: 0.9030\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.4310 - acc: 0.8053 - val_loss: 0.2950 - val_acc: 0.9000\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.4291 - acc: 0.8011 - val_loss: 0.2979 - val_acc: 0.8939\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.4286 - acc: 0.8067 - val_loss: 0.3023 - val_acc: 0.8909\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.4281 - acc: 0.8025 - val_loss: 0.2905 - val_acc: 0.8970\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.4267 - acc: 0.8039 - val_loss: 0.2967 - val_acc: 0.8939\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.4260 - acc: 0.8123 - val_loss: 0.3004 - val_acc: 0.8848\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.4257 - acc: 0.8123 - val_loss: 0.2970 - val_acc: 0.8879\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.4252 - acc: 0.8081 - val_loss: 0.2882 - val_acc: 0.8939\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.4249 - acc: 0.8123 - val_loss: 0.2929 - val_acc: 0.8939\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.4247 - acc: 0.8109 - val_loss: 0.2916 - val_acc: 0.8939\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.4236 - acc: 0.8137 - val_loss: 0.2950 - val_acc: 0.8939\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.4233 - acc: 0.8123 - val_loss: 0.2980 - val_acc: 0.8848\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.4230 - acc: 0.8151 - val_loss: 0.2967 - val_acc: 0.8848\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.4223 - acc: 0.8151 - val_loss: 0.2972 - val_acc: 0.8788\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.4220 - acc: 0.8165 - val_loss: 0.3036 - val_acc: 0.8667\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.4218 - acc: 0.8165 - val_loss: 0.2929 - val_acc: 0.8848\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.4207 - acc: 0.8193 - val_loss: 0.3009 - val_acc: 0.8697\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.4208 - acc: 0.8151 - val_loss: 0.3013 - val_acc: 0.8727\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.4206 - acc: 0.8207 - val_loss: 0.3122 - val_acc: 0.8455\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.4199 - acc: 0.8193 - val_loss: 0.3017 - val_acc: 0.8606\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.4209 - acc: 0.8207 - val_loss: 0.3049 - val_acc: 0.8576\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.4199 - acc: 0.8165 - val_loss: 0.3031 - val_acc: 0.8606\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.4196 - acc: 0.8165 - val_loss: 0.3030 - val_acc: 0.8667\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.4191 - acc: 0.8221 - val_loss: 0.3020 - val_acc: 0.8667\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.4188 - acc: 0.8179 - val_loss: 0.3031 - val_acc: 0.8636\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.4186 - acc: 0.8207 - val_loss: 0.3041 - val_acc: 0.8606\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.4186 - acc: 0.8179 - val_loss: 0.3025 - val_acc: 0.8606\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.4179 - acc: 0.8221 - val_loss: 0.2984 - val_acc: 0.8606\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.4180 - acc: 0.8193 - val_loss: 0.2998 - val_acc: 0.8667\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.4184 - acc: 0.8221 - val_loss: 0.3087 - val_acc: 0.8515\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.4176 - acc: 0.8193 - val_loss: 0.2953 - val_acc: 0.8727\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.4181 - acc: 0.8207 - val_loss: 0.2950 - val_acc: 0.8727\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.4180 - acc: 0.8221 - val_loss: 0.2963 - val_acc: 0.8697\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.4171 - acc: 0.8221 - val_loss: 0.3028 - val_acc: 0.8576\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.4170 - acc: 0.8221 - val_loss: 0.3045 - val_acc: 0.8576\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.4169 - acc: 0.8207 - val_loss: 0.3021 - val_acc: 0.8606\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.4169 - acc: 0.8235 - val_loss: 0.2997 - val_acc: 0.8667\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.4164 - acc: 0.8207 - val_loss: 0.3016 - val_acc: 0.8606\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.4160 - acc: 0.8207 - val_loss: 0.3021 - val_acc: 0.8636\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.4169 - acc: 0.8193 - val_loss: 0.3078 - val_acc: 0.8515\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.4156 - acc: 0.8235 - val_loss: 0.3046 - val_acc: 0.8606\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.4164 - acc: 0.8221 - val_loss: 0.2999 - val_acc: 0.8667\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.4154 - acc: 0.8235 - val_loss: 0.3028 - val_acc: 0.8667\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.4150 - acc: 0.8249 - val_loss: 0.3018 - val_acc: 0.8636\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.4152 - acc: 0.8235 - val_loss: 0.2952 - val_acc: 0.8727\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.4149 - acc: 0.8235 - val_loss: 0.2985 - val_acc: 0.8727\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.4143 - acc: 0.8291 - val_loss: 0.2990 - val_acc: 0.8667\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.4142 - acc: 0.8263 - val_loss: 0.3036 - val_acc: 0.8576\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.4151 - acc: 0.8221 - val_loss: 0.2966 - val_acc: 0.8697\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.4140 - acc: 0.8207 - val_loss: 0.2993 - val_acc: 0.8697\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.4138 - acc: 0.8263 - val_loss: 0.3052 - val_acc: 0.8424\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.4138 - acc: 0.8249 - val_loss: 0.3067 - val_acc: 0.8424\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.4137 - acc: 0.8235 - val_loss: 0.3063 - val_acc: 0.8394\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.4136 - acc: 0.8221 - val_loss: 0.2980 - val_acc: 0.8667\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.4131 - acc: 0.8207 - val_loss: 0.3030 - val_acc: 0.8455\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.4133 - acc: 0.8249 - val_loss: 0.3026 - val_acc: 0.8455\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.4129 - acc: 0.8249 - val_loss: 0.2954 - val_acc: 0.8667\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.4127 - acc: 0.8263 - val_loss: 0.2980 - val_acc: 0.8636\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.4124 - acc: 0.8235 - val_loss: 0.3000 - val_acc: 0.8606\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.4130 - acc: 0.8235 - val_loss: 0.2986 - val_acc: 0.8455\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.4121 - acc: 0.8249 - val_loss: 0.2995 - val_acc: 0.8455\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.4130 - acc: 0.8207 - val_loss: 0.3141 - val_acc: 0.8364\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.4123 - acc: 0.8221 - val_loss: 0.2965 - val_acc: 0.8636\n",
      "Epoch 81/200\n",
      " - 0s - loss: 0.4123 - acc: 0.8207 - val_loss: 0.3016 - val_acc: 0.8394\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.4116 - acc: 0.8263 - val_loss: 0.3004 - val_acc: 0.8455\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.4116 - acc: 0.8263 - val_loss: 0.3051 - val_acc: 0.8455\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.4117 - acc: 0.8221 - val_loss: 0.3013 - val_acc: 0.8606\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.4117 - acc: 0.8221 - val_loss: 0.3063 - val_acc: 0.8424\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.4114 - acc: 0.8249 - val_loss: 0.2970 - val_acc: 0.8636\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.4110 - acc: 0.8277 - val_loss: 0.2999 - val_acc: 0.8485\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.4116 - acc: 0.8235 - val_loss: 0.2996 - val_acc: 0.8485\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.4112 - acc: 0.8221 - val_loss: 0.2965 - val_acc: 0.8485\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.4105 - acc: 0.8263 - val_loss: 0.3087 - val_acc: 0.8364\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.4107 - acc: 0.8263 - val_loss: 0.2970 - val_acc: 0.8485\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.4115 - acc: 0.8263 - val_loss: 0.3072 - val_acc: 0.8424\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.4102 - acc: 0.8235 - val_loss: 0.2954 - val_acc: 0.8455\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.4101 - acc: 0.8207 - val_loss: 0.2992 - val_acc: 0.8424\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.4099 - acc: 0.8277 - val_loss: 0.3043 - val_acc: 0.8394\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.4113 - acc: 0.8221 - val_loss: 0.2977 - val_acc: 0.8606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/200\n",
      " - 0s - loss: 0.4097 - acc: 0.8277 - val_loss: 0.3032 - val_acc: 0.8424\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.4103 - acc: 0.8151 - val_loss: 0.3056 - val_acc: 0.8424\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.4091 - acc: 0.8263 - val_loss: 0.2942 - val_acc: 0.8606\n",
      "Epoch 100/200\n",
      " - 0s - loss: 0.4096 - acc: 0.8263 - val_loss: 0.2993 - val_acc: 0.8424\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.4098 - acc: 0.8207 - val_loss: 0.3026 - val_acc: 0.8424\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.4095 - acc: 0.8277 - val_loss: 0.2940 - val_acc: 0.8455\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.4101 - acc: 0.8291 - val_loss: 0.2977 - val_acc: 0.8485\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.4086 - acc: 0.8235 - val_loss: 0.2933 - val_acc: 0.8485\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.4095 - acc: 0.8263 - val_loss: 0.2909 - val_acc: 0.8455\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.4086 - acc: 0.8277 - val_loss: 0.3077 - val_acc: 0.8364\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.4084 - acc: 0.8235 - val_loss: 0.3023 - val_acc: 0.8424\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.4081 - acc: 0.8221 - val_loss: 0.2966 - val_acc: 0.8455\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.4085 - acc: 0.8207 - val_loss: 0.2993 - val_acc: 0.8424\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.4080 - acc: 0.8235 - val_loss: 0.3000 - val_acc: 0.8424\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.4079 - acc: 0.8249 - val_loss: 0.2940 - val_acc: 0.8515\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.4081 - acc: 0.8249 - val_loss: 0.2987 - val_acc: 0.8455\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.4085 - acc: 0.8263 - val_loss: 0.3039 - val_acc: 0.8394\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.4073 - acc: 0.8249 - val_loss: 0.2981 - val_acc: 0.8455\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.4075 - acc: 0.8263 - val_loss: 0.3019 - val_acc: 0.8394\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.4092 - acc: 0.8263 - val_loss: 0.2976 - val_acc: 0.8424\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.4074 - acc: 0.8207 - val_loss: 0.3007 - val_acc: 0.8424\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.4068 - acc: 0.8235 - val_loss: 0.2949 - val_acc: 0.8485\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.4070 - acc: 0.8235 - val_loss: 0.2940 - val_acc: 0.8606\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.4072 - acc: 0.8277 - val_loss: 0.3018 - val_acc: 0.8424\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.4075 - acc: 0.8249 - val_loss: 0.3099 - val_acc: 0.8364\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.4077 - acc: 0.8249 - val_loss: 0.2957 - val_acc: 0.8485\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.4070 - acc: 0.8277 - val_loss: 0.3006 - val_acc: 0.8455\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.4074 - acc: 0.8235 - val_loss: 0.3080 - val_acc: 0.8394\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.4067 - acc: 0.8235 - val_loss: 0.3055 - val_acc: 0.8394\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.4060 - acc: 0.8235 - val_loss: 0.2936 - val_acc: 0.8515\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.4068 - acc: 0.8249 - val_loss: 0.2955 - val_acc: 0.8606\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.4062 - acc: 0.8333 - val_loss: 0.2889 - val_acc: 0.8545\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.4057 - acc: 0.8277 - val_loss: 0.3022 - val_acc: 0.8424\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.4070 - acc: 0.8249 - val_loss: 0.3064 - val_acc: 0.8394\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.4057 - acc: 0.8277 - val_loss: 0.3028 - val_acc: 0.8455\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.4054 - acc: 0.8221 - val_loss: 0.3043 - val_acc: 0.8455\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.4059 - acc: 0.8235 - val_loss: 0.2942 - val_acc: 0.8636\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.4058 - acc: 0.8263 - val_loss: 0.3130 - val_acc: 0.8364\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.4057 - acc: 0.8221 - val_loss: 0.2931 - val_acc: 0.8636\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.4052 - acc: 0.8291 - val_loss: 0.3040 - val_acc: 0.8455\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.4046 - acc: 0.8263 - val_loss: 0.3017 - val_acc: 0.8455\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.4037 - acc: 0.8263 - val_loss: 0.2999 - val_acc: 0.8606\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.4041 - acc: 0.8305 - val_loss: 0.2990 - val_acc: 0.8636\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.4039 - acc: 0.8277 - val_loss: 0.3085 - val_acc: 0.8576\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.4041 - acc: 0.8221 - val_loss: 0.3081 - val_acc: 0.8424\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.4028 - acc: 0.8249 - val_loss: 0.3015 - val_acc: 0.8485\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.4026 - acc: 0.8263 - val_loss: 0.2966 - val_acc: 0.8636\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.4030 - acc: 0.8235 - val_loss: 0.3000 - val_acc: 0.8636\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.4021 - acc: 0.8277 - val_loss: 0.2936 - val_acc: 0.8727\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.4022 - acc: 0.8263 - val_loss: 0.3039 - val_acc: 0.8515\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.4014 - acc: 0.8235 - val_loss: 0.2961 - val_acc: 0.8727\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.4017 - acc: 0.8235 - val_loss: 0.3023 - val_acc: 0.8667\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.4011 - acc: 0.8221 - val_loss: 0.3016 - val_acc: 0.8606\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.4004 - acc: 0.8249 - val_loss: 0.2941 - val_acc: 0.8758\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.3996 - acc: 0.8235 - val_loss: 0.3007 - val_acc: 0.8636\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.4001 - acc: 0.8207 - val_loss: 0.3009 - val_acc: 0.8667\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.3991 - acc: 0.8221 - val_loss: 0.3009 - val_acc: 0.8727\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.3995 - acc: 0.8291 - val_loss: 0.2899 - val_acc: 0.8697\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.3979 - acc: 0.8221 - val_loss: 0.3108 - val_acc: 0.8545\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.3980 - acc: 0.8235 - val_loss: 0.3039 - val_acc: 0.8515\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.3975 - acc: 0.8305 - val_loss: 0.3069 - val_acc: 0.8545\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.3977 - acc: 0.8221 - val_loss: 0.2937 - val_acc: 0.8667\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.3963 - acc: 0.8235 - val_loss: 0.3037 - val_acc: 0.8576\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.3960 - acc: 0.8249 - val_loss: 0.3041 - val_acc: 0.8515\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.3967 - acc: 0.8305 - val_loss: 0.2948 - val_acc: 0.8667\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.3958 - acc: 0.8249 - val_loss: 0.3018 - val_acc: 0.8606\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.3967 - acc: 0.8235 - val_loss: 0.3012 - val_acc: 0.8576\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.3954 - acc: 0.8277 - val_loss: 0.3037 - val_acc: 0.8485\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.3950 - acc: 0.8263 - val_loss: 0.2949 - val_acc: 0.8606\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.3949 - acc: 0.8249 - val_loss: 0.3042 - val_acc: 0.8606\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.3958 - acc: 0.8319 - val_loss: 0.3042 - val_acc: 0.8545\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.3942 - acc: 0.8235 - val_loss: 0.2997 - val_acc: 0.8485\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.3949 - acc: 0.8249 - val_loss: 0.3025 - val_acc: 0.8545\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.3954 - acc: 0.8291 - val_loss: 0.3031 - val_acc: 0.8576\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.3939 - acc: 0.8277 - val_loss: 0.3016 - val_acc: 0.8545\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.3940 - acc: 0.8277 - val_loss: 0.2963 - val_acc: 0.8576\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.3949 - acc: 0.8235 - val_loss: 0.2984 - val_acc: 0.8515\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.3947 - acc: 0.8263 - val_loss: 0.3097 - val_acc: 0.8485\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.3947 - acc: 0.8291 - val_loss: 0.3075 - val_acc: 0.8424\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.3943 - acc: 0.8221 - val_loss: 0.3056 - val_acc: 0.8485\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.3925 - acc: 0.8249 - val_loss: 0.2983 - val_acc: 0.8545\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.3918 - acc: 0.8277 - val_loss: 0.2978 - val_acc: 0.8455\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.3926 - acc: 0.8263 - val_loss: 0.3032 - val_acc: 0.8455\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.3915 - acc: 0.8249 - val_loss: 0.2955 - val_acc: 0.8606\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.3919 - acc: 0.8249 - val_loss: 0.3009 - val_acc: 0.8455\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.3912 - acc: 0.8263 - val_loss: 0.3008 - val_acc: 0.8606\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.3916 - acc: 0.8263 - val_loss: 0.3065 - val_acc: 0.8545\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.3916 - acc: 0.8277 - val_loss: 0.3075 - val_acc: 0.8455\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.3910 - acc: 0.8277 - val_loss: 0.3009 - val_acc: 0.8545\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.3910 - acc: 0.8277 - val_loss: 0.3030 - val_acc: 0.8545\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.3904 - acc: 0.8277 - val_loss: 0.3050 - val_acc: 0.8455\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.3903 - acc: 0.8305 - val_loss: 0.3123 - val_acc: 0.8394\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.3912 - acc: 0.8319 - val_loss: 0.3085 - val_acc: 0.8394\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.3907 - acc: 0.8277 - val_loss: 0.2967 - val_acc: 0.8485\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.3904 - acc: 0.8333 - val_loss: 0.3135 - val_acc: 0.8394\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.3897 - acc: 0.8291 - val_loss: 0.3070 - val_acc: 0.8394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/200\n",
      " - 0s - loss: 0.3896 - acc: 0.8277 - val_loss: 0.3091 - val_acc: 0.8424\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.3904 - acc: 0.8375 - val_loss: 0.3132 - val_acc: 0.8394\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.3917 - acc: 0.8333 - val_loss: 0.2939 - val_acc: 0.8485\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.3896 - acc: 0.8291 - val_loss: 0.3086 - val_acc: 0.8394\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.3885 - acc: 0.8333 - val_loss: 0.3014 - val_acc: 0.8424\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.3893 - acc: 0.8305 - val_loss: 0.3129 - val_acc: 0.8424\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.3884 - acc: 0.8277 - val_loss: 0.3028 - val_acc: 0.8394\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.3883 - acc: 0.8263 - val_loss: 0.3054 - val_acc: 0.8485\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1efa01acf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initializing the Neural Network\n",
    "classifier = Sequential()\n",
    "# First hidden layer\n",
    "classifier.add(Dense(units=9, activation='relu', kernel_initializer='uniform', input_shape=(df_train.shape[1],)))\n",
    "# Second hidden layer\n",
    "classifier.add(Dense(units=9, activation='relu', kernel_initializer='uniform', input_shape=(df_train.shape[1],)))\n",
    "# Output layer\n",
    "classifier.add(Dense(1, activation='sigmoid', input_shape=(df_train.shape[1],)))\n",
    "\n",
    "# compile method: Configures the model for training\n",
    "classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit method: Trains the model for a fixed number of epochs (iterations on a dataset)\n",
    "history = classifier.fit(df_train, df_target, batch_size=32, epochs=200, verbose=2, validation_data=(df_test, df_target_pred2), shuffle=True)\n",
    "\n",
    "# plot history to see how our loss error changes with each epoch\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.ylabel(\"error\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluating the network\n",
    "y_pred = classifier.predict(df_test)\n",
    "y_final = (y_pred > 0.5).astype(int).reshape(df_test.shape[0])\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': df_target_pred['PassengerId'], 'Survived': y_final})\n",
    "output.to_csv('prediction-ann.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.848%\n"
     ]
    }
   ],
   "source": [
    "# Retrieve prediction data\n",
    "predictions = pd.read_csv('prediction-ann.csv')\n",
    "# Need to reset indexing for Dataframe because some rows were removed initially and some index have been removed\n",
    "df_target_pred = df_target_pred.reset_index(drop=True)\n",
    "\n",
    "correct_pred = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions['Survived'][i] == df_target_pred['Survived'][i]:\n",
    "        correct_pred += 1\n",
    "accuracy = round((correct_pred/len(predictions))*100,3)\n",
    "print(\"Accuracy: \" + str(accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "Given an accuracy rate of 84.848%, we can be rather confident of our prediction for the survivability of each passenger, given the parameters as specified above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
